# Databricks SparkFlow Analytics ğŸ“ŠğŸ’¸  

![Databricks](https://img.shields.io/badge/Platform-Databricks-orange?logo=databricks)  
![PySpark](https://img.shields.io/badge/PySpark-ETL-blue?logo=apachespark)  
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-Storage-brightgreen)  
![Spark SQL](https://img.shields.io/badge/Spark%20SQL-Analytics-purple?logo=apachespark)  
![Tableau](https://img.shields.io/badge/Tableau-Visualization-red?logo=tableau)  
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

A **modern data lakehouse & analytics solution** built on **Databricks**, powered by **PySpark, Spark SQL, and Delta Lake** with **automated workflows** for scalable, reliable pipeline execution, seamlessly integrated with **Tableau** to deliver compelling and actionable **business visualizations**.



This project demonstrates **end-to-end data engineering and BI practices**:  

âœ… Scalable **data lakehouse** design  
âœ… Automated **ETL pipelines** with **Databricks Workflows**  
âœ… **Star schema modeling** for analytics  
âœ… **Actionable insights** through **Spark SQL** based reporting and **Tableau visualizations**

---

## ğŸ–¥ Data Architecture â€“ Medallion Approach  

The project adopts the **Medallion Architecture** with three layers:  

| Layer   | Purpose |
|---------|---------|
| ğŸ¥‰ **Bronze** | Raw ingested data from ERP/CRM CSV files with **incremental loading** for efficient processing. |
| ğŸ¥ˆ **Silver** | Cleaned & standardized data ensuring quality. |
| ğŸ¥‡ **Gold**   | Business-ready, star schema data for BI & reporting  with **robust data validation**. |


## ğŸ—ï¸ Medallion Architecture Diagram


![Medallion Architecture](./data_lakehouse/schema_documentation/data_lakehouse_project_architecture.png)


---

## Pipeline Automation ğŸš€

This project includes **automated workflows** in **Databricks** to orchestrate the **Medallion Architecture ETL pipeline**, **data quality checks**, and **advanced analytics**. The workflow ensures seamless execution from raw data ingestion (Bronze) to business-ready insights (Gold), with error handling and logging.


![Databricks Workflow Automation](./data_lakehouse/schema_documentation/databricks_workflow_automation.png)

### Workflow Details
**Bronze Layer**: **Incremental loading** efficiently processes large-scale ERP/CRM data, reducing costs and enabling near real-time updates. Ingests CSV data for tables like crm_cust_info, crm_prd_info, crm_sales_details, erp_loc_a101, erp_cust_az12, and erp_px_cat_g1v2 using **Delta Lake MERGE**. Key features:

- **Upsert** via unique keys (merge_key).
- **Deduplication** using load_timestamp or columns like cst_create_date.
- Error handling and load duration logging.

**Silver Layer**: Cleans, transforms, and quality-checks data for consistency.

**Gold Layer**: Builds star schema tables (dim_customers, dim_products, fact_sales) with robust validation via GoldenLayerDataValidation class, ensuring:

- No null values for data integrity.
- Deduplication on keys like cst_id, prd_id.

Analytics: Performs exploratory and advanced analytics.

This automation ensures scalability and reliability for **production-grade data pipelines**.


---

## ğŸ“– Project Highlights  

- ğŸª™ **Data Architecture** â€“ Medallion layers with **Delta Lake storage** for reliable data management.
- ğŸª™ **Incremental Loading** â€“ Optimizes Bronze layer ingestion by processing only new or updated data, reducing compute overhead and enabling scalable **Automated ETL pipelines**.
- ğŸª™ **Robust Data Validation** â€“ Utilizes GoldenLayerDataValidation class in the Gold layer to ensure clean, deduplicated data for accurate analytics and reporting.
- ğŸª™ **Automated ETL Pipelines** â€“ Built in **PySpark & Spark SQL**.  
- ğŸª™ **Data Modeling** â€“ Fact & dimension tables in a **star schema**.  

---

## ğŸ—‚ Repository Structure  

ğŸ“‚ **data_lakehouse**/

â”£ ğŸ“‚ **datasource**/ â†’ Raw ERP & CRM CSV files

â”£ ğŸ“‚ **schema_documentation**/ â†’ Data model & schema documentations

â”£ ğŸ“‚ **ETL_scripts**/ â†’ ETL code (bronze_layer.py, incremental_bronze.py, silver_layer.py, gold_layer.py)

â”£ ğŸ“‚ **data_quality_checks**/ â†’ Data quality & pipeline validation



ğŸ“‚ **data_analytics**/

â”£ ğŸ“‚ **analytics_scripts** / â†’ exploratory_data_analysis (EDA) & advance_analytics

â”£ ğŸ“‚ **analytics_data_source** / â†’ (gold.dim_customers.csv, gold.dim_products.csv, gold.fact_sales.csv)

â”£ ğŸ“‚ **data_analytics_roadmap** / â†’ data_analytics_roadmap

ğŸ“‚ **data_visualisation/**  

â”£ ğŸ“„ **business_performance.twbx** â†’ Tableau packaged workbook for business performance dashboard  
â”£ ğŸ–¼ï¸ **business_performance.png** â†’ Snapshot of the visualization  


---

## ğŸ¯ Target Audience  

This project is designed for **data engineers, analysts, and students** showcasing expertise in:  

- ğŸ **PySpark & Spark SQL Development**  
- ğŸ—ï¸ **Data Lakehouse with Medallion Architecture**  
- âš™ï¸ **Automated ETL Pipeline Engineering** 
- â­ **Star Schema Data Modeling** 
- ğŸ“Š **Data Analytics & BI** with **Tableau**

---

## ğŸ“Š Business Insights  

Analytics & reporting focus on:  

- ğŸ‘¥ **Customer Behavior** â€“ Segmentation (VIP, Regular, New), retention, churn.  
- ğŸ“¦ **Product Performance** â€“ Category contribution, sales vs. costs.  
- ğŸ“… **Sales Trends** â€“ Seasonal patterns, regional metrics, growth tracking.  

These insights are visualized through **Tableau** dashboards, supporting **strategic business decisions**.  


  ### Business Performance Dashboard Example: 

![Business Performance Dashboard](./data_visualization/business_performance.png)

---

## ğŸ›  Technologies Used  

- ğŸ”§ **Databricks** â€“ Unified data platform  
- ğŸ **PySpark** â€“ Scalable automated ETL pipelines  
- ğŸ“œ **Spark SQL** â€“ Transformations & analytics  
- ğŸ’¾ **Delta Lake** â€“ Reliable, versioned storage
- ğŸ“Š **Tableau** â€“ Visualization for business insights


---

## ğŸ›¡ï¸ License  

Licensed under the **MIT License**.  

---

âœ¨ With SparkFlow Analytics, raw ERP & CRM data is transformed into a **scalable, analytics-ready lakehouse** that powers **data-driven business insights** through **PySpark**, **Spark SQL**, **Delta Lake**, and **Tableau visualizations**.

